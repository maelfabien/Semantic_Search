{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Semantic Search</h1></center>\n",
    "<center> - </center>\n",
    "<center> Exploring Embedding techniques and Similarity Measures </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The task is to develop a semantic search engine. We have a corpus of texts. Since there is a large amount of documents, we want to run small search queries so that is returns texts that are semantically close to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process can be illustrated the following way :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/process.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T23:07:29.846573Z",
     "start_time": "2019-03-27T23:07:23.324652Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.21.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "### General ###\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt; plt.rcdefaults()\n",
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "### Text processing ###\n",
    "from nltk import wordpunct_tokenize, WordNetLemmatizer, sent_tokenize, pos_tag\n",
    "from nltk.corpus import stopwords as sw, wordnet as wn\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import string \n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "### Pre-trained model ###\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "### Live Search Engine ###\n",
    "from IPython.core.magic import (register_line_magic, register_cell_magic,\n",
    "                                register_line_cell_magic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-27T23:07:30.955089Z",
     "start_time": "2019-03-27T23:07:29.848835Z"
    }
   },
   "outputs": [],
   "source": [
    "EN = spacy.load('en')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elon Musk's Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To replicate the kind of messages we would observe in a message app, I have chosen to explore Elon Musk's tweets data set : https://data.world/adamhelsinger/elon-musk-tweets-until-4-6-17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:25:13.585786Z",
     "start_time": "2019-03-28T16:25:13.500127Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    b'And so the robots spared humanity ... https:...\n",
       "1    b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...\n",
       "2        b'@waltmossberg @mims @defcon_5 Et tu, Walt?'\n",
       "3                  b'Stormy weather in Shortville ...'\n",
       "4    b\"@DaveLeeBBC @verge Coal is dying due to nat ...\n",
       "5    b\"@Lexxxzis It's just a helicopter in helicopt...\n",
       "6                            b\"@verge It won't matter\"\n",
       "7                        b'@SuperCoolCube Pretty good'\n",
       "8    b\"Why did we waste so much time developing sil...\n",
       "9    b'Technology breakthrough: turns out chemtrail...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = pd.read_csv('tweets.csv')['text']\n",
    "tweets.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:25:14.658792Z",
     "start_time": "2019-03-28T16:25:14.651106Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2819,)"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### South Park Series Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T21:58:37.018917Z",
     "start_time": "2019-03-28T21:58:36.808278Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           You guys, you guys! Chef is going away. \\n\n",
       "1                          Going away? For how long?\\n\n",
       "2                                           Forever.\\n\n",
       "3                                    I'm sorry boys.\\n\n",
       "4    Chef said he's been bored, so he joining a gro...\n",
       "Name: Line, dtype: object"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "south = pd.read_csv('All-seasons.csv')['Line']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-28T22:01:22.652Z"
    }
   },
   "outputs": [],
   "source": [
    "south.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# I . Pre-processing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to tokenize the text :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T21:58:26.941745Z",
     "start_time": "2019-03-28T21:58:26.828265Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess(document, max_features = 150, max_sentence_len = 300):\n",
    "    \"\"\"\n",
    "    Returns a normalized, lemmatized list of tokens from a document by\n",
    "    applying segmentation (breaking into sentences), then word/punctuation\n",
    "    tokenization, and finally part of speech tagging. It uses the part of\n",
    "    speech tags to look up the lemma in WordNet, and returns the lowercase\n",
    "    version of all the words, removing stopwords and punctuation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def lemmatize(token, tag):\n",
    "        \"\"\"\n",
    "        Converts the tag to a WordNet POS tag, then uses that\n",
    "        tag to perform an accurate WordNet lemmatization.\n",
    "        \"\"\"\n",
    "        tag = {\n",
    "        'N': wn.NOUN,\n",
    "        'V': wn.VERB,\n",
    "        'R': wn.ADV,\n",
    "        'J': wn.ADJ\n",
    "        }.get(tag[0], wn.NOUN)\n",
    "\n",
    "        return WordNetLemmatizer().lemmatize(token, tag)\n",
    "\n",
    "    def vectorize(doc, max_features, max_sentence_len):\n",
    "        \"\"\"\n",
    "        Converts a document into a sequence of indices of length max_sentence_len retaining only max_features unique words\n",
    "        \"\"\"\n",
    "        tokenizer = Tokenizer(num_words=max_features)\n",
    "        tokenizer.fit_on_texts(doc)\n",
    "        doc = tokenizer.texts_to_sequences(doc)\n",
    "        doc_pad = pad_sequences(doc, padding = 'pre', truncating = 'pre', maxlen = max_sentence_len)\n",
    "        return np.squeeze(doc_pad), tokenizer.word_index\n",
    "\n",
    "    lemmatized_tokens = []\n",
    "\n",
    "    # Clean the text using a few regular expressions\n",
    "    document = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", document)\n",
    "    document = re.sub(r\"what's\", \"what is \", document)\n",
    "    document = re.sub(r\"\\'\", \" \", document)\n",
    "    document = re.sub(r\"@\", \" \", document)\n",
    "    document = re.sub(r\"\\'ve\", \" have \", document)\n",
    "    document = re.sub(r\"can't\", \"cannot \", document)\n",
    "    document = re.sub(r\"n't\", \" not \", document)\n",
    "    document = re.sub(r\"i'm\", \"i am \", document)\n",
    "    document = re.sub(r\"\\'re\", \" are \", document)\n",
    "    document = re.sub(r\"\\'d\", \" would \", document)\n",
    "    document = re.sub(r\"\\'ll\", \" will \", document)\n",
    "    document = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", document)\n",
    "    document = re.sub(r\"\\n\", \" \", document)\n",
    "    \n",
    "    cleaned_document = []\n",
    "    \n",
    "    # Break the document into sentences\n",
    "    for sent in sent_tokenize(document):\n",
    "\n",
    "        # Break the sentence into part of speech tagged tokens\n",
    "        for token, tag in pos_tag(wordpunct_tokenize(sent)):\n",
    "\n",
    "            # Apply preprocessing to the tokens\n",
    "            token = token.lower()\n",
    "            token = token.strip()\n",
    "            token = token.strip('_')\n",
    "            token = token.strip('*')\n",
    "\n",
    "            # If punctuation or stopword, ignore token and continue\n",
    "            if token in set(sw.words('english')) or all(char in set(string.punctuation) for char in token):\n",
    "                continue\n",
    "\n",
    "            # Lemmatize the token\n",
    "            lemma = lemmatize(token, tag)\n",
    "            lemmatized_tokens.append(lemma)\n",
    "\n",
    "        cleaned_document.append(' '.join(lemmatized_tokens))\n",
    "    \n",
    "    \n",
    "    #vectorized_document, word_index = vectorize(cleaned_document, max_features, max_sentence_len)\n",
    "    #return vectorized_document, word_index\n",
    "    return cleaned_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T16:26:13.747266Z",
     "start_time": "2019-03-28T16:25:51.251362Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets = preprocess(str(list(tweets)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-03-28T21:59:06.304Z"
    }
   },
   "outputs": [],
   "source": [
    "south = preprocess(str(list(south)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "south"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# II . Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Rolling Window Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Sentence Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a. Test the embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the USE module\n",
    "module_url = \"https://tfhub.dev/google/universal-sentence-encoder-large/3\" \n",
    "embed = hub.Module(module_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:32:27.203660Z",
     "start_time": "2019-03-28T12:32:25.001367Z"
    }
   },
   "outputs": [],
   "source": [
    "# Generate the embedding and print out some descriptive data\n",
    "def generate_embedding(messages):\n",
    "    with tf.Session() as session:\n",
    "        session.run([tf.global_variables_initializer(), tf.tables_initializer()])\n",
    "        message_embeddings = session.run(embed(messages))\n",
    "    for i, message_embedding in enumerate(np.array(message_embeddings).tolist()):\n",
    "        print(\"Message: {}\".format(messages[i]))\n",
    "        print(\"Embedding size: {}\".format(len(message_embedding)))\n",
    "        message_embedding_snippet = \", \".join(\n",
    "            (str(x) for x in message_embedding[:3]))\n",
    "        print(\"Embedding: [{}, ...]\\n\".format(message_embedding_snippet))\n",
    "    return(message_embeddings[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to generate a first embedding :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T12:54:27.944827Z",
     "start_time": "2019-03-28T12:54:11.743126Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0328 13:54:23.967081 4613125568 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Message: How can I reset my password\n",
      "Embedding size: 512\n",
      "Embedding: [0.025553924962878227, -0.034720830619335175, 0.0020717347506433725, ...]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emb = generate_embedding([\"How can I reset my password\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Embed the whole file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T13:48:49.303233Z",
     "start_time": "2019-03-28T13:48:27.379893Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1645: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "session = tf.InteractiveSession()\n",
    "session.run(tf.global_variables_initializer())\n",
    "session.run(tf.tables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:38:48.951596Z",
     "start_time": "2019-03-28T15:38:46.586644Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0328 16:38:48.509640 4613125568 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "\n",
    "# For evaluation we use exactly normalized rather than\n",
    "# approximately normalized.\n",
    "sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\n",
    "\n",
    "def get_embeds(session, text):\n",
    "    \"\"\"Returns the similarity scores\"\"\"\n",
    "    embed = session.run(\n",
    "        [sts_encode1],\n",
    "        feed_dict={\n",
    "            sts_input1: text\n",
    "        })\n",
    "    return(embed[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:40:49.311047Z",
     "start_time": "2019-03-28T15:38:49.576768Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 µs, sys: 1e+03 ns, total: 4 µs\n",
      "Wall time: 8.82 µs\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "embed_tweets = []\n",
    "\n",
    "for message in range(len(tweets)) :\n",
    "    embed_tweets.append(get_embeds(session, [tweets[message]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:44:03.336541Z",
     "start_time": "2019-03-28T15:44:03.306166Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'And so the robots spared humanity ... https:...</td>\n",
       "      <td>[0.05683216, -0.019019788, 0.11236075, -0.0346...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
       "      <td>[0.01255778, 0.019348582, 0.09177675, -0.01283...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "      <td>[0.008331334, -0.010440288, -0.011671914, -0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Stormy weather in Shortville ...'</td>\n",
       "      <td>[-0.046504013, 0.01973891, 0.0043140077, -0.02...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
       "      <td>[0.028700273, -0.027458383, 0.09349278, 0.0341...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  b'And so the robots spared humanity ... https:...   \n",
       "1  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...   \n",
       "2      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'   \n",
       "3                b'Stormy weather in Shortville ...'   \n",
       "4  b\"@DaveLeeBBC @verge Coal is dying due to nat ...   \n",
       "\n",
       "                                               embed  \n",
       "0  [0.05683216, -0.019019788, 0.11236075, -0.0346...  \n",
       "1  [0.01255778, 0.019348582, 0.09177675, -0.01283...  \n",
       "2  [0.008331334, -0.010440288, -0.011671914, -0.0...  \n",
       "3  [-0.046504013, 0.01973891, 0.0043140077, -0.02...  \n",
       "4  [0.028700273, -0.027458383, 0.09349278, 0.0341...  "
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df = pd.DataFrame(tweets)\n",
    "tweets_df['embed'] = embed_tweets\n",
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:54:59.871980Z",
     "start_time": "2019-03-28T15:54:59.822466Z"
    }
   },
   "outputs": [],
   "source": [
    "tweets_df['embed'] = tweets_df['embed'].apply(lambda x : x.astype('float'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:55:06.554533Z",
     "start_time": "2019-03-28T15:55:06.522740Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>embed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'And so the robots spared humanity ... https:...</td>\n",
       "      <td>[0.05683216080069542, -0.01901978813111782, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...</td>\n",
       "      <td>[0.012557780370116234, 0.01934858225286007, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'@waltmossberg @mims @defcon_5 Et tu, Walt?'</td>\n",
       "      <td>[0.008331334218382835, -0.010440288111567497, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'Stormy weather in Shortville ...'</td>\n",
       "      <td>[-0.04650401324033737, 0.01973891071975231, 0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b\"@DaveLeeBBC @verge Coal is dying due to nat ...</td>\n",
       "      <td>[0.028700273483991623, -0.02745838277041912, 0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  b'And so the robots spared humanity ... https:...   \n",
       "1  b\"@ForIn2020 @waltmossberg @mims @defcon_5 Exa...   \n",
       "2      b'@waltmossberg @mims @defcon_5 Et tu, Walt?'   \n",
       "3                b'Stormy weather in Shortville ...'   \n",
       "4  b\"@DaveLeeBBC @verge Coal is dying due to nat ...   \n",
       "\n",
       "                                               embed  \n",
       "0  [0.05683216080069542, -0.01901978813111782, 0....  \n",
       "1  [0.012557780370116234, 0.01934858225286007, 0....  \n",
       "2  [0.008331334218382835, -0.010440288111567497, ...  \n",
       "3  [-0.04650401324033737, 0.01973891071975231, 0....  \n",
       "4  [0.028700273483991623, -0.02745838277041912, 0...  "
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Define semantic similarity metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:55:13.896534Z",
     "start_time": "2019-03-28T15:55:10.483236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0328 16:55:13.300189 4613125568 tf_logging.py:115] Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "sts_input1 = tf.placeholder(tf.string, shape=(None))\n",
    "sts_encode2 = tf.placeholder(tf.float32)\n",
    "\n",
    "# For evaluation we use exactly normalized rather than\n",
    "# approximately normalized.\n",
    "sts_encode1 = tf.nn.l2_normalize(embed(sts_input1), axis=1)\n",
    "\n",
    "cosine_similarities = tf.reduce_sum(tf.multiply(sts_encode1, sts_encode2), axis=1)\n",
    "clip_cosine_similarities = tf.clip_by_value(cosine_similarities, 0.0, 1.0)\n",
    "sim_scores = 1.0 - tf.divide(tf.acos(clip_cosine_similarities), 3.14)\n",
    "\n",
    "def get_scores(session, text_a, text_b):\n",
    "    \"\"\"Returns the similarity scores\"\"\"\n",
    "    scores= session.run(\n",
    "        [sim_scores],\n",
    "        feed_dict={\n",
    "            sts_input1: text_a,\n",
    "            sts_encode2: text_b\n",
    "        })\n",
    "    return(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:57:03.148742Z",
     "start_time": "2019-03-28T15:57:03.141406Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_results(sessions, sentence, num):\n",
    "    examples = [e for e in tweets_df['embed']]\n",
    "    scores = get_scores(session, [sentence], examples)\n",
    "    tweets_df['cosine'] = scores[0].tolist()\n",
    "    return(tweets_df.sort_values('cosine', ascending=False).head(n=num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:57:03.658878Z",
     "start_time": "2019-03-28T15:57:03.651910Z"
    }
   },
   "outputs": [],
   "source": [
    "def print_res(test, num=20):\n",
    "    res = get_results(session, test, num).round(4)\n",
    "    res = (res.set_index('cosine'))\n",
    "    print('{}\\n'.format(test))\n",
    "    print('\\x1b[31mScore{:<1} \\x1b[0m: \\x1b[34m Matching sentence\\x1b[0m'.format(''))\n",
    "    for i in res.iterrows():\n",
    "        print('\\x1b[31m{:<6} \\x1b[0m: \\x1b[0m \\x1b[34m{}\\x1b[0m'.format(i[0], i[1][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T15:58:36.626438Z",
     "start_time": "2019-03-28T15:58:36.538121Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "space launch\n",
      "\n",
      "\u001b[31mScore  \u001b[0m: \u001b[34m Matching sentence\u001b[0m\n",
      "\u001b[31m0.774  \u001b[0m: \u001b[0m \u001b[34mb\"Can't delay any longer. Must proceed with primary mission to launch the Deep Space Climate Observatory spacecraft.\"\u001b[0m\n",
      "\u001b[31m0.7663 \u001b[0m: \u001b[0m \u001b[34mb'High velocity reentry (2700 lbs/sqft) appeared to succeed, but, as expected, not enough propellant to land for this and the next mission.'\u001b[0m\n",
      "\u001b[31m0.7613 \u001b[0m: \u001b[0m \u001b[34mb'Primary mission on target. Spacecraft head towards the sun! All good there.'\u001b[0m\n",
      "\u001b[31m0.7563 \u001b[0m: \u001b[0m \u001b[34mb'Dragon captured by the International Space Station! Just awesome ... http://t.co/ihoZqgj7'\u001b[0m\n",
      "\u001b[31m0.7562 \u001b[0m: \u001b[0m \u001b[34mb'Drone spaceport ship heads to its hold position in the Atlantic to prepare for a rocket landing http://t.co/kXYHGVKTfE'\u001b[0m\n",
      "\u001b[31m0.7542 \u001b[0m: \u001b[0m \u001b[34mb'Jeff maybe unaware SpaceX suborbital VTOL flight began 2013. Orbital water landing 2014. Orbital land landing next. https://t.co/S6WMRnEFY5'\u001b[0m\n",
      "\u001b[31m0.7524 \u001b[0m: \u001b[0m \u001b[34mb'Counting down to the first SpaceX launch from the Apollo 11 launch pad tomorrow morning (webcast\\xe2\\x80\\xa6 https://t.co/upjTcmftte'\u001b[0m\n",
      "\u001b[31m0.751  \u001b[0m: \u001b[0m \u001b[34mb\"Next landing attempt will be 3rd launch from now. Tonight's flight and following one will not have enough propellant.\"\u001b[0m\n",
      "\u001b[31m0.7507 \u001b[0m: \u001b[0m \u001b[34mb'Spaceship has departed from the International Space Station. Firing thrusters to deorbit in ~30 mins #Dragon'\u001b[0m\n",
      "\u001b[31m0.7506 \u001b[0m: \u001b[0m \u001b[34mb'RT @SpaceX: Falcon 9 first stage has landed at LZ-1'\u001b[0m\n",
      "\u001b[31m0.7488 \u001b[0m: \u001b[0m \u001b[34mb'Dragon 2 is designed to be able to land anywhere in the solar system. Red Dragon Mars mission is the first test flight.'\u001b[0m\n",
      "\u001b[31m0.748  \u001b[0m: \u001b[0m \u001b[34mb'Ascent phase good. Dragon deployed to Space Station rendezvous orbit.'\u001b[0m\n",
      "\u001b[31m0.7475 \u001b[0m: \u001b[0m \u001b[34mb'About 110 miles away and the spacecraft is now in direct communication with the Space Station #Dragon'\u001b[0m\n",
      "\u001b[31m0.7468 \u001b[0m: \u001b[0m \u001b[34mb'Ascent successful. Dragon enroute to Space Station. Rocket landed on droneship, but too hard for survival.'\u001b[0m\n",
      "\u001b[31m0.7463 \u001b[0m: \u001b[0m \u001b[34mb'@Jon_Favreau @SpaceX Thanks!'\u001b[0m\n",
      "\u001b[31m0.7463 \u001b[0m: \u001b[0m \u001b[34mb'Short vid of the recent @SpaceX mission to the Intl Space Station http://t.co/Qc963d8B'\u001b[0m\n",
      "\u001b[31m0.7461 \u001b[0m: \u001b[0m \u001b[34mb'First of next gen Falcon 9 rockets rolls out to the launch pad at Vandenberg Air Force Base http://t.co/hNl6zKodvr'\u001b[0m\n",
      "\u001b[31m0.7461 \u001b[0m: \u001b[0m \u001b[34mb'Hold down firing of Dragon 2 spacecraft at Cape Canaveral http://t.co/2626wb8596'\u001b[0m\n",
      "\u001b[31m0.7449 \u001b[0m: \u001b[0m \u001b[34mb'@stunt_penguin @LayOn_OverWhale Stage 1 would reach low Earth orbit if not hefting a second stage, but payload would be greatly reduced.'\u001b[0m\n",
      "\u001b[31m0.7439 \u001b[0m: \u001b[0m \u001b[34mb'RT @NASA: The @SpaceX #Dragon capsule passed directly below the #ISS at a distance of 2.5km, fulfilling all demonstration objectives for ...'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print_res(\"space launch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Live search prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T21:37:46.529083Z",
     "start_time": "2019-03-28T21:37:46.522229Z"
    }
   },
   "outputs": [],
   "source": [
    "@register_cell_magic\n",
    "def search(line, cell):\n",
    "    return print_res(cell)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-28T21:37:07.132517Z",
     "start_time": "2019-03-28T21:37:06.289877Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Messages on twitter\"\n",
      "\n",
      "\u001b[31mScore  \u001b[0m: \u001b[34m Matching sentence\u001b[0m\n",
      "\u001b[31m0.7404 \u001b[0m: \u001b[0m \u001b[34mb'@apple_defense @samabuelsamid Exactly! I love Twitter.'\u001b[0m\n",
      "\u001b[31m0.7214 \u001b[0m: \u001b[0m \u001b[34mb\"@tonykatz Don't like having a zillion tweets in the log. Makes it tough to wade through if someone wants to read my tweet history.\"\u001b[0m\n",
      "\u001b[31m0.7182 \u001b[0m: \u001b[0m \u001b[34mb'Single character Tweets are the ulitmate extension of the Twitmeme...'\u001b[0m\n",
      "\u001b[31m0.7155 \u001b[0m: \u001b[0m \u001b[34mb'Not easy to convey irony in a tweet'\u001b[0m\n",
      "\u001b[31m0.7091 \u001b[0m: \u001b[0m \u001b[34mb'@SwiftOnSecurity I like your tweets!'\u001b[0m\n",
      "\u001b[31m0.7001 \u001b[0m: \u001b[0m \u001b[34mb'Please ignore prior tweets, as that was someone pretending to be me :)  This is actually me.'\u001b[0m\n",
      "\u001b[31m0.6972 \u001b[0m: \u001b[0m \u001b[34mb'Signing off now. That was more than enough Twitter trouble for one morning!'\u001b[0m\n",
      "\u001b[31m0.6838 \u001b[0m: \u001b[0m \u001b[34mb'@BullFlags Yeah. And Twitter is a hater Hellscape.'\u001b[0m\n",
      "\u001b[31m0.6716 \u001b[0m: \u001b[0m \u001b[34mb'@YostRobert @StephenAtHome Yeah, and several others at various times. My twitter list is trimmed down to almost all news/comedy these days.'\u001b[0m\n",
      "\u001b[31m0.669  \u001b[0m: \u001b[0m \u001b[34mb'Wow, this tweet is like a Rorschach test'\u001b[0m\n",
      "\u001b[31m0.6647 \u001b[0m: \u001b[0m \u001b[34mb\"Had a minor operation, so am in bed in meds. Probably shouldn't be tweeting :)\"\u001b[0m\n",
      "\u001b[31m0.6538 \u001b[0m: \u001b[0m \u001b[34mb'SpaceX announcement tomorrow at 1pm PST'\u001b[0m\n",
      "\u001b[31m0.6536 \u001b[0m: \u001b[0m \u001b[34mb'Tesla announcement goes live at 5pm California time. 30 minutes of media Q&amp;A to follow.'\u001b[0m\n",
      "\u001b[31m0.6529 \u001b[0m: \u001b[0m \u001b[34mb'@iamDeveloper Really? Ok, I will work harder on thinking first and tweeting second. Sorry for anything thoughtless.'\u001b[0m\n",
      "\u001b[31m0.6502 \u001b[0m: \u001b[0m \u001b[34mb'@FifthRocket Did you read my tweets? I attacked it hard. Did not defend.'\u001b[0m\n",
      "\u001b[31m0.645  \u001b[0m: \u001b[0m \u001b[34mb'@lonelysandwich needs latest update'\u001b[0m\n",
      "\u001b[31m0.6412 \u001b[0m: \u001b[0m \u001b[34mb'@mjasandoval Stay tuned'\u001b[0m\n",
      "\u001b[31m0.6372 \u001b[0m: \u001b[0m \u001b[34mb'Meant to reply to this article: https://t.co/JRvXcTG3el'\u001b[0m\n",
      "\u001b[31m0.6369 \u001b[0m: \u001b[0m \u001b[34mb'Except for the tweet about large amounts of crack (actually just small amounts)'\u001b[0m\n",
      "\u001b[31m0.6367 \u001b[0m: \u001b[0m \u001b[34mb'@loic thanks Loic :)'\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "%%search\n",
    "\"Messages on twitter\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sources :\n",
    "- https://github.com/choran/sentence_embeddings"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
